\documentclass{article}
\usepackage{arxiv}

\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[square,numbers]{natbib}
\usepackage{doi}
\usepackage{color}
\usepackage{xcolor}

\newcommand{\TODO}[1]{\textcolor{purple}{ToDo: #1.}}

\captionsetup[figure]{labelfont={bf},name={Fig.},labelsep=period}
\captionsetup[table]{labelfont={bf},name={Table},labelsep=period, skip=5pt}
\addto\captionsrussian{\def\refname{References}}
\bibliographystyle{abbrvnat}

  
\title{Efficient Image Super Resolution Using Attention In Binary Neural Networks}

\author{Zharikov Ilya \\
	Moscow Institute of Physics and Technology\\
	%% examples of more authors
	\And
	Kirill Ovcharenko \\
	Moscow Institute of Physics and Technology\\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Image Super Resolution [SR] is a crucial class of image processing techniques that enhance quality of visual data. 
Deep Convolutional Neural Networks [DCNN] have recently shown great results in this field. However, application of DCNN on resource-limited devices remains challenging, as they demand significant amounts of memory, energy and computations. Binary neural networks [BNN] provide a promising approach to reduce computational complexity and speed up the inference of a model. To our best knowledge, there are not many papers devoted to applying BNN to SR tasks, as SR models are much more vulnerable to degradation in performance when decreasing the precision of weights, than image classification models. The paper proposes modification of a convolutional block to make it binary without suffering severe performance decrease.

\end{abstract}


\keywords{Binary Neural Network \and Single Image Super Resolution \and Binarization \and Model compression}

\section{Introduction}

\TODO{Need to be extended in terms related works further}

Image Super Resolution [SR]~\cite{wang2020deep} aims to restore High Resolution [HR] image from corrupted Low Resolution [LR] counterpart. This task is important because of its various applications in medical imaging~\cite{dharejo2022multimodal} and surveillance instruments~\cite{aakerberg2022real}. In spite of the research in this field being active, it has not progressed a lot, as some challenges were encountered. Main obstacle is desired output in SR tasks being much more diverse than input, so the model is required to do dense pixel-level prediction, hence is bound to be more complex.  

Recent advances in the field of SR owe their success to Deep Convolutional Neural Networks [DCNN] which show state-of-the-art results in a wide range of computer vision problems, such as image classification, Semantic Segmentation etc. However, the models solving these tasks are usually complicated and demand a lot of space and computational resources, thus hindering their implementation on mobile devices, drones and other machines which are limited in GPU memory.

Lately, different methods of reducing complexity of these models were proposed. While some papers focus on pruning and knowledge distillation, other researches introduce quantization as a way to decrease memory needed.

The most extreme form of quantization is binarization. Binary Neural Networks [BNN] use only $1$ bit to represent each parameter, thus drastically decreasing space demanded to store the model. Moreover, with all parameters of the model set to $\{-1, 1\}$, most of the the calculations can be conducted using XNOR and Bitcount operations. This approach seems promising, as it proposes new ways to design hardware that can help to handle and exploit complex neural networks.

However, it is obvious that BNN sacrifice precision and quality, as they have much less capacity and representational potential than Full-Precision [FP] networks. Previous works in this field propose different methods of maintaining competitive accuracy while achieving better performance. The paper~\cite{ma2019efficient} focuses on residual block binarization, which helps to reduce a significant part of the model's parameters. However, full-precision activations keep computational complexity of the model pretty high. ReactNet~\cite{liu2020reactnet} suggests generalized binarization and activations functions that help to shift distribution, which significantly increases representational capacity of the binary model. The BBCU~\cite{xia2022basic} proposed effective Convolutional Unit that can be used in any architecture that relies on residual connections. It provides much more efficient training and inference, but oversimplifies weight binarization. Moreover, the block modification implies disposal of the batch normalization block. However, recent studies~\cite{wei2023ebsr} show that models without the BatchNorm module (e.g. EDSR~\cite{lim2017enhanced}) suffer a performance drop during binarization as they have diverse distributions between channels and layers. 
IR-Net~\cite{qin2020forward} reduces information loss by balancing weights to achieve maximum information entropy in forward propagation. After that, IR2Net~\cite{xue2022ir2net} proposes two essential components of the learning process: Information Restriction and Information Recovery. BNext~\cite{guo2022join} also applies attention mechanism to obtain the key information from the full-precision activations and smooth out the loss landscape. 
However, last two papers investigate only the impact of these methods on performance of Image Classification models. Another way of extracting necessary information was proposed in~\cite{hu2018squeeze}, where a squeeze-and-excitation block is added to every transformation to a feature map, so that it can learn dependencies between channels (which are expected to concentrate on different features). 
In contrast to regular approaches, the paper~\cite{zhao2020efficient} presented a new method of attention that helps model to better get pixel-level dependencies and exhibits great results in SR task. 

This paper adopts some techniques from the researches, mentioned above, and suggests further modifications of convolutional block that help to improve BNN's performance in SR tasks. 

(1) We adopt the idea of using generalized activation function from \cite{liu2020reactnet}. That helps to achieve better performance while preserving reasonable computational demands. 

(2) Batch Normalization block was proven to decrease quality of restored images~\cite{lim2017enhanced}. However, spreading the distribution of binary block's output is necessary for more uniform information flow, as shown in ~\cite{xia2022basic}. Thus, we apply scaling block to the output of binary convolution.

(3) Finally, we advance the idea from ~\cite{xue2022ir2net} and ~\cite{guo2022join} to restrict information from the input to increase learning productivity by implementing attention modules into our binary network. We try different ways to compute attention maps~\ref{blocks}, including the methods proposed in ~\cite{zhao2020efficient} and ~\cite{hu2018squeeze}.

\section{Problem statement}
\label{sec:headings}

\begin{figure}[t]
\centering
  \begin{subfigure}[c]{0.35\textwidth}
    \includegraphics[width=\linewidth]{se_attn.pdf}
    \caption{Squeeze-and-excitation attention} \label{blocks:a}
  \end{subfigure}%
  \hspace*{\fill}   % maximize separation between the subfigures
  \begin{subfigure}[c]{0.3\textwidth}
    \includegraphics[width=\linewidth]{spatial_attn.pdf}
    \caption{Spatial attention block} \label{blocks:b}
  \end{subfigure}%
  \hspace*{\fill}   % maximizeseparation between the subfigures
  \begin{subfigure}[c]{0.31\textwidth}
    \includegraphics[width=\linewidth]{global_attn.pdf}
    \caption{Global attention block} \label{blocks:c}
  \end{subfigure}

\caption{Attention blocks} \label{blocks}
\end{figure}

\TODO{Need to be slightly reformulated during theory week}

Let $\{(X_i, Y_i)\}_{i=1}^n$ be our image dataset, where $X_i \in \mathbb{R}^{h_i \times w_i \times 3}$ denotes the low resolution image and $Y_i \in \mathbb{R}^{H_i \times W_i \times 3}$ is the high resolution one. Considering $M$ to be the model, SR task targets optimization of 
\begin{equation}
    Q(M) = \frac{1}{n}\sum\limits_{i=1}^nf(\mathbf{M}(\mathbf{X_i}), \mathbf{Y_i})
\end{equation}
where $f$ represents either $\text{PSNR}$ or $\text{SSIM}$ metric, defined as:
\begin{equation}
    \text{PSNR}(x, y) = 10 \log_{10} \left(\frac{\text{MAX}_I^2}{\text{MSE}(x, y)}\right)
\end{equation}


Here $\text{MAX}_I$ is the maximum valid value for pixel, $\text{MSE}$ is mean squared error.

\begin{equation}
    \text{SSIM}(x, y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_{x}^2 + \sigma_{y}^2 + c_2)}
\end{equation}

$\mu_x$ denotes the mean for $x$, $\mu_y$ is the mean for $y$, $\sigma_x$ is the variance for $x$, $\sigma_y$ is the variance for $y$, $\sigma_{xy}$ is the covariation of $x$ and $y$, $c_1$ and $c_2$ - two constants depending on dynamic pixel range.

Now let $B \in \mathcal{B}$ be our binarized representation of model $M$. $\mathcal{B}$ is space of BNN. Assuming $L$ to be number of layers in $B \in \mathcal{B}$, $W_l \in \{-1, 1\}^{C_{out} \times C_{in} \times K_h \times K_w}$, $l \in \{1 \ , ... \ , L\}$. Here $C_{out}$ is the number of output channels, $C_{in}$ is the number of input channels, $K_h$ is the kernel height, $K_w$ is the kernel weight. 


Thus, the problem of binarization can be expressed in finding $B^{*}$ as
\begin{equation}
    B^{*} = \arg\min\limits_{B \in \mathcal{B}} \left[Q(M) - Q(B)\right]
\end{equation}

\TODO{Add specific optimization task of finding the optimal convolutional block structure}

\section{Basic Binary Convolutional Block modification}
\subsection{Baseline block}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.8\textwidth}
    \centering
   \includegraphics[width=1\linewidth]{base.pdf}
   \caption{Base block}
   \label{block:a} 
\end{subfigure}
~
\begin{subfigure}[b]{0.8\textwidth}
    \centering
   \includegraphics[width=1\linewidth]{base_resid.pdf}
   \caption{Base residual block}
   \label{block:b} 
\end{subfigure}
\end{figure}

In this section we define basic binarization operations that are used to build the Binary Convolutional Block.

Let $X_t^f \in \mathbb{R}^{H \times W \times C_{in}}$ and $W_t^f \in \mathbb{R}^{K_{h} \times K_{w} \times C_{in} \times C_{out}}$ be full-precision activations and full-precision convolution weights on the $t$-th layer respectively. Here $H$ and $W$ denote the input feature map height and width, $C_{in}$ stands for the number of input channels and $C_{out}$ is the number of output channels. Then $X_t^b \in \{-1, 1\}^{H \times W \times C_{in}}$, $W_t^b \in \{-1, 1\}^{K_{h} \times K_{w} \times C_{in} \times C_{out}}$ would be the binary approximations for the corresponding full-precision parameters.

When using binary parameters, convolution operation $X^b \ast W^b$ can be effectively performed using XNOR and Bitcount operations:

\begin{equation*}
    X_t^b \ast W_t^b = Bitcount(XNOR(X_t^b, W_t^b))
\end{equation*}

We use regular sign function for activations. Thus, the binary representations can be acquired as follows:

\begin{equation}
    x_{i, j, k}^b = Sign(x_{i, j, k}^f) = 
    \begin{cases}
        +1, & x_{i, j, k}^f > 0 \\
        -1, & x_{i, j, k}^f \le 0
    \end{cases}, \quad
    i \in [0, H), j \in [0, W), k \in [0, C_{in})
\label{eq_act_sign}
\end{equation}

Here $x_{i, j, k}^f \in X_t^f$, $x_{i, j, k}^b \in X_t^b$ are single full-precision and binary activations respectively.

Obviously, the derivative of the Sign function cannot be utilized in the training process, as it is impossible to propagate gradients through it. We use an approximation of the Sign derivative, which is defined as follows:

\begin{equation}
    Approx\left(\dfrac{\partial Sign(x_{i, j, k})}{\partial x_{i, j, k}}\right) = 
    \begin{cases}
        2 + 2x_{i, j, k}, & - 1 \le x_{i, j, k}^f < 0 \\
        2 - 2x_{i, j, k}, & 0 \le x_{i, j, k}^f < +1 \\
        0, & otherwise
    \end{cases}, \quad
    i \in [0, H), j \in [0, W), k \in [0, C_{in})
\label{eq_act_sign_der}
\end{equation}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{rescale_pre.pdf}
   \caption{RescalePre block}
   \label{block:c} 
\end{subfigure}

\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{rescale_post.pdf}
   \caption{RescalePost block}
   \label{block:d} 
\end{subfigure}
\end{figure}

When binarizing the convolution weights we use the scaled Sign function:

\begin{equation}
    w_{i, j, k, l}^b = Sign(w_{i, j, k, l}^f) = 
    \begin{cases}
        +\alpha_l, & w_{i, j, k, l}^f > 0 \\
        -\alpha_l, & w_{i, j, k, l}^f \le 0
    \end{cases}, \quad
    i \in [0, H), j \in [0, W), k \in [0, C_{in}), l \in [0, C_{out})
\end{equation}

Here full-precision and binary weights are denoted as $w_{i, j, k, l}^f \in W_t^f$, $w_{i, j, k, l}^b \in W_t^b$ and $\alpha_l \in \mathbb{R}$ represents the scale factor.

Optimization task of finding the optimal scale factor for binary weights can be expressed as follows:
\begin{equation}
    \alpha_l^* = \arg\min\limits_{\alpha} ||W_{t, l}^f - \alpha W_{t, l}^b||
\end{equation}

Where $W_{t, l}^f \in \mathbb{R}^{K_{h} \times K_{w} \times C_{in}}$, $W_{t, l}^b \in \{-1, 1\}^{K_{h} \times K_{w} \times C_{in}}$. We use optimal value $\alpha_l = \alpha^* = \dfrac{|W_{t, l}^f|}{n}$ to avoid overcomplicating the training process.

We use RPReLU~\cite{liu2020reactnet} as activation function, because it achieves better performance by shifting the negative component of input's distribution, which is important for BNNs. RPReLU is defined as follows:

\begin{equation}
    RPReLU(x_{i, j, k}) = 
    \begin{cases}
        x_{i, j, k} - \gamma_{k} + \zeta_{k}, & x_{i, j, k} > \gamma_{k} \\
        \beta_k(x_{i, j, k} - \gamma_k) + \zeta_k, & x_{i, j, k} \le \gamma_k
    \end{cases}, \quad
    i \in [0, H), j \in [0, W), k \in [0, C_{in})
\end{equation}

Where $x_{i, j, k} \in \mathbb{R}^{H \times W \times C_{in}}$ is an element of the input feature map, $\gamma_k \in \mathbb{R}$ and $\zeta_k \in \mathbb{R}$ are learnable shifts for moving the distribution, and $\beta_k \in \mathbb{R}$ is a learnable coefficient controlling the slope of the negative part.

Previous researches displayed the importance of the residual connection in the Binary Convolution Block, especially in the SR task, so we keep it for every binary convolution to transfer the full-precision information through the block. Moreover, BBCU~\cite{xia2022basic} shows that activation function narrows the negative part of the residual connection, thus losing negative full-precision information. On that account, we keep the idea of moving the residual connection out of the activation function.

EDSR~\cite{lim2017enhanced} showed that applying Batch Normalization has a negative impact on quality when dealing with pixel-level tasks, such as SR. But the experiments conducted in ~\cite{xia2022basic} show that spreading the distribution of values is necessary for BNNs. For that reason, we introduce scale block that applies linear transformation to the output of binary convolution.

Taking in consideration all the points mentioned above, the baseline block can be expressed as follows:

\begin{equation}
    X_{t+1}^f = RPReLU(a_t \cdot (X_{t}^b \ast W_t^b) + b_t) + X_t^f
\end{equation}

Here $a_t \in \mathbb{R}$ and $b_t \in \mathbb{R}$ denote learnable Scale block parameters.

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{single.pdf}
   \caption{Single block}
   \label{block:e} 
\end{subfigure}

\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{dual.pdf}
   \caption{Dual block}
   \label{block:f} 
\end{subfigure}
\end{figure}

\subsection{Attention modules}
When the full-precision model is being binarized, it is bound to lose some representational capacity and suffer a performance decrease. Previous researches~\cite{guo2022join, xue2022ir2net} focus on applying attention mechanism to help the model to capture the most important features and dependencies. Further advancing the idea of restricting information, we suggest attention modules that help the model to extract necessary features from the input. 

We propose several different attention mechanisms: a simple squeeze-and-excitation attention, a spatial attention and a global attention block.

A squeeze-and-excitation block~\cite{hu2018squeeze} is depicted on \ref{blocks:a}. It consists of Global Average Pooling (GAP), two linear layers with a non-linear activation function (ReLU) and a sigmoid function, which is defined as follows:

\begin{equation*}
    \sigma(x) = \dfrac{1}{1 + e^{-x}}
\end{equation*}

\begin{figure}[t]
\includegraphics[width=1\linewidth]{both.pdf}
   \caption{Both block}
   \label{block:g} 
\end{figure}

The structure of the block helps model to learn non-linear dependencies between channels, hence between different features.

Spatial attention block (\ref{blocks:b}) consists of one $1\times1$ convolution with $1$ output channel followed by a sigmoid function. It focuses on getting a 2D attention map which helps it to learn relationships between channels for every pixel on the feature map.

Finally, global attention block (\ref{blocks:c}) has similar structure to the spatial attention block, except for applying Global Average Pooling before a $1 \times 1$ convolution. It helps to capture inter-channel dependencies with no regard to the pixel position.

\subsection{Proposed modifications}
Base and Base-Residual blocks are depicted on ~\ref{block:a} and ~\ref{block:b} respectively. We propose several modifications that aim to either restrict information from the previous layer or change the distribution of binary convolution inputs and outputs.

Firstly, we investigate the importance of rescaling the inputs and outputs of the binary convolution, proposing RescalePre and RescalePost blocks, that are displayed on ~\ref{block:c} and ~\ref{block:d} correspondingly. 

The outputs of the convolution branch can also contain unnecessary information that should not be propagated to the next layer. Thus, we add attention block that is applied to the RPReLU output before connecting with residual information, getting the modification that is referred to as Single Block~\ref{block:e}.

\begin{table}[t]
    \centering
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
     \hline
      \multirow{2}{*}{Modification} & \multirow{2}{*}{GFLOPs} & \multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14} & \multicolumn{2}{c}{B100} & \multicolumn{2}{c}{Urban100} \vline\\ 
      
        & & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ 
        EDSR & 45.1 & $38.05$ & $0.978$ & $33.57$ & $0.966$ & $32.19$ & $0.978$ & $32.04$ & $0.911$ \\
        Base & $5.97$ & $37.61$ & $0.975$ & $33.09$ & $0.963$ & $31.85$ & $0.977$ & $30.79$ & $0.905$ \\
        Base-Residual & $5.97$ & $37.60$ & $0.976$ & $33.07$ & $0.963$ & $31.85$ & $0.977$ & $30.80$ & $0.905$ \\
        RescalePre-Global & $8.11$ & $37.55$ & $0.976$ & $33.03$ & $0.963$ & $31.82$ & $0.977$ & $30.74$ & $0.904$ \\
        RescalePre-SE & $8.12$ & $37.32$ & $0.974$ & $32.83$ & $0.962$ & $31.64$ & $0.976$ & $30.14$ & $0.902$ \\
        RescalePre-Spatial & $8.14$ & $37.58$ & $0.976$ & $33.04$ & $0.963$ & $31.80$ & $0.977$ & $30.68$ & $0.905$ \\
        RescalePost-Global & $8.12$ & $37.65$ & $0.976$ & $33.12$ & $0.963$ & $31.88$ & $0.977$ & $30.88$ & $0.906$ \\
        RescalePost-SE & $8.12$ & $37.57$ & $0.976$ & $33.06$ & $0.963$ & $31.82$ & $0.977$ & $30.82$ & $0.906$ \\
        RescalePost-Spatial & $8.14$ & $37.63$ & $0.976$ & $33.12$ & $0.964$ & $31.86$ & $0.977$ & $30.86$ & $0.906$ \\
        Single-Spatial & $8.14$ & $37.64$ & $\mathbf{0.977}$ & $33.11$ & $0.964$ & $31.87$ & $0.977$ & $30.84$ & $0.906$ \\
        Single-SE & $8.12$ & $37.62$ & $0.976$ & $33.08$ & $0.963$ & $31.85$ & $0.977$ & $30.87$ & $0.906$ \\
        Single-Global & $8.11$ & $37.62$ & $0.976$ & $33.12$ & $0.963$ & $31.87$ & $0.977$ & $30.85$ & $0.906$ \\
        Dual-SE-Spatial & $10.29$ & $37.61$ & $0.976$ & $33.09$ & $0.963$ & $31.84$ & $0.977$ & $30.83$ & $0.906$ \\
        Dual-Global-Spatial & $10.28$ & $37.65$ & $\mathbf{0.977}$ & $33.09$ & $0.964$ & $31.85$ & $0.977$ & $30.85$ & $0.907$ \\
        Dual-SE-Global & $10.26$ & $37.57$ & $\mathbf{0.977}$ & $33.07$ & $0.964$ & $31.84$ & $0.977$ & $30.83$ & $0.906$ \\
        Both-SE-SE & $10.27$ & $37.60$ & $0.975$ & $33.10$ & $0.963$ & $31.87$ & $0.977$ & $31.00$ & $0.906$ \\
        Both-SE-Spatial & $10.29$ & $37.63$ & $0.976$ & $33.15$ & $0.964$ & $31.89$ & $0.977$ & $31.02$ & $0.907$ \\
        Both-Spatial-SE & $10.29$ & $\mathbf{37.68}$ & $0.976$ & $33.17$ & $0.964$ & $\mathbf{31.91}$ & $\mathbf{0.978}$ & $\mathbf{31.08}$ & $\mathbf{0.908}$ \\
        Both-Spatial-Spatial & $10.31$ & $\mathbf{37.68}$ & $0.976$ & $\mathbf{33.18}$ & $\mathbf{0.965}$ & $31.88$ & $0.977$ & $30.93$ & $0.906$ \\
        Both-Global-Global & $10.25$ & $37.66$ & $\mathbf{0.977}$ & $33.17$ & $0.964$ & $\mathbf{31.91}$ & $0.977$ & $31.01$ & $0.907$ \\
        Both-Global-Spatial & $10.28$ & $37.62$ & $\mathbf{0.977}$ & $33.13$ & $0.964$ & $31.89$ & $0.977$ & $30.90$ & $0.906$ \\
        Both-Spatial-Global & $10.28$ & $37.65$ & $\mathbf{0.977}$ & $33.16$ & $\mathbf{0.965}$ & $31.89$ & $0.977$ & $30.91$ & $0.907$ \\
        Both-Global-SE & $10.26$ & $37.62$ & $0.975$ & $33.13$ & $0.964$ & $31.88$ & $0.977$ & $31.02$ & $\mathbf{0.908}$ \\
        Both-SE-Global & $10.26$ & $37.63$ & $\mathbf{0.977}$ & $33.13$ & $\mathbf{0.965}$ & $31.87$ & $0.977$ & $31.00$ & $0.907$ \\
        BBCU & $6.0$ & $37.67$ & $0.976$ & $33.14$ & $0.964$ & $31.89$ & $0.977$ & $30.92$ & $0.906$ \\

     \hline
    \end{tabular}
    \caption{Quantitative comparison of PSNR and SSIM on benchmark datasets}
    \label{tab:metric_tab}
\end{table}

Dual Block~\ref{block:f} explores the possibility of combining different attention mechanisms to extract various features from the binary information.

In the baseline block, outputs of the binary convolution and activations from the previous layer are added to each other and have the same influence on the result. However, full-precision residual connections have information that cannot be effectively processed by the binary convolution due to its simple structure. Therefore, we need to provide a method to get the most important features from previous layer. Hence, we add an attention module to the residual branch of the convolutional block, obtaining the most  advanced modification - Both Block~\ref{block:g}.

Having similar structure with regular convolutional block, our modification can be implemented in any SR model architecture with minimal effort. In the next section we apply the modified block to EDSR and prove it to show state-of-the-art results on the benchmark datasets.

\section{Computational experiment}
\subsection{Experiment setup}

We use EDSR~\cite{lim2017enhanced} as a backbone for our modified binary block, as it doesn't require batch normalization module and shows state-of-the-art results in Super-Resolution task. Each convolution of the body is replaced with a modified binary block.

We train our models on DIV2K~\cite{agustsson2017ntire} dataset, which contains 800 images. We evaluate models on 4 benchmark datasets: Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2012single}, B100~\cite{martin2001database} and Urban100~\cite{huang2015single}. 

\TODO{Add training setup (batch size, number of epochs etc.)}

\subsection{Experiment results}
Quantitative results are presented in the table~\ref{tab:metric_tab}. The Both-Spatial-SE block shows the best performance among other modifications.

\subsection{Ablation study}

\begin{table}[t]
\centering
    \begin{subtable}{.5\linewidth}
      \centering
        \begin{tabular}{ |c|c|c| } 
         \hline
          \multirow{2}{*}{Modification} & \multicolumn{2}{c}{Urban100} \vline\\ 
          
            & PSNR & SSIM \\ 
            Single-Spatial & $30.84$ & $0.906$ \\
            Single-SE & $30.87$ & $0.906$ \\
            Single-Global & $30.85$ & $0.906$ \\
            Dual-SE-Spatial & $30.83$ & $0.906$ \\
            Dual-Global-Spatial & $30.85$ & $0.907$ \\
            Dual-SE-Global & $30.83$ & $0.906$ \\
    
         \hline
        \end{tabular}
    \subcaption{Single block VS Dual block}
    \label{res:table-a}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \begin{tabular}{ |c|c|c| } 
         \hline
          \multirow{2}{*}{Modification} & \multicolumn{2}{c}{Urban100} \vline\\ 
          
            & PSNR & SSIM \\ 
            RescalePre-Global & $30.74$ & $0.904$ \\
            RescalePre-SE & $30.14$ & $0.902$ \\
            RescalePre-Spatial & $30.68$ & $0.905$ \\
            RescalePost-Global & $30.88$ & $0.906$ \\
            RescalePost-SE & $30.82$ & $0.906$ \\
            RescalePost-Spatial & $30.86$ & $0.906$ \\
        
         \hline
        \end{tabular}
    \subcaption{RescalePre block VS RescalePost block}
    \label{res:table-b}
    \end{subtable}
    \begin{subtable}{.5\linewidth}
      \centering
        \begin{tabular}{ |c|c|c| } 
         \hline
          \multirow{2}{*}{Modification} & \multicolumn{2}{c}{Urban100} \vline\\ 
          
            & PSNR & SSIM \\ 
            RescalePost-Global & $30.88$ & $0.906$ \\
            RescalePost-SE & $30.82$ & $0.906$ \\
            RescalePost-Spatial & $30.86$ & $0.906$ \\
            Single-Spatial & $30.84$ & $0.906$ \\
            Single-SE & $30.87$ & $0.906$ \\
            Single-Global & $30.85$ & $0.906$ \\
        
         \hline
        \end{tabular}
    \subcaption{RescalePost block VS Single block}
    \label{res:table-c}
    \end{subtable}
    \caption{Quantitative comparison on Urban100 dataset}
    \label{res:table}
\end{table}

Quantitative results~\ref{res:table-a} demonstrate that applying two different attention mechanisms to the output simultaneously (Dual block) performs worse than just applying one mechanism (Single block). The reason behind this is the different purpose of the attention blocks: spatial attention aims to learn inter-channel dependencies for each pixel, while global and SE-attention disregard the spatial position. Combining these blocks can overcomplicate the information and slow down training process.

Comparison of the RescalePre and RescalePost results~\ref{res:table-b} is consistent with previous researches on the importance of scaling the output of the binary branch. RescalePre block doesn't affect forward pass of the model when using regular Sign function, having influence only on the backward pass. On the other hand, RescalePost block prevents overlapping of the information between two branches, similar to scaling in BBCU~\cite{xia2022basic}. Thus, the RescalePost modification shows better performance by providing more uniform flow of binary and full-precision information.

Contrast between the results of RescalePost and Single blocks~\ref{res:table-c} displays the importance of applying attention based on the full-precision input. RescalePost block performs rescaling based on the result of the binary block itself, having no direct impact on the input of the block. Yet Single block constructs attention map from the input, thus restricting unnecessary information before propagating further into the block. 

Prevalence of the Both block performance shows that both residual and binary branches require information restriction based on the full-precision input.

\clearpage
\section{Conclusion}
This paper proposes several binary block modifications that improve the performance of Binary Neural Networks in Super Resolution task. We consider attention module being an essential component of the BNN, as it helps to restrict the unnecessary information and provides the network with more effective training process. Quantitative results prove this hypothesis and show that binary neural networks actually suffer from abundance of the input information and perform better when being restricted. 

Future researches can be aimed at implementing different attention blocks into the BNN and further experiments on restricting information and learning channel-wise and pixel-wise distribution variations.

\bibliography{references}

\end{document}